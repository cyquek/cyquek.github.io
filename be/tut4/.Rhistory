knitr::opts_chunk$set(echo = TRUE)
# Read in the data:
t12 <-read.csv("homes.csv")
# View first few rows of df
head(t12)
# Use the str() function to view the df structure as well as its columns' classes.
str(t12)
# Create the time-series variable:
H <- ts(t12$HOMES, start = c(1992,1), end = c(2010,3), frequency = 12)
IR <- ts(t12$IRATE, start = c(1992,1), end = c(2010,3), frequency = 12)
# Create first differences of H and IR:
DH <- diff(H)
DIR <- diff(IR)
# Time series plots of H, DH, IR and DIR:
plot(H)
plot(DH)
plot(IR)
plot(DIR)
library(CADFtest)
# Unit root test for H, IR, DH and DIR
adf.H <-CADFtest(H, type="drift", criterion="MAIC", max.lag.y=8)
adf.IR <-CADFtest(IR, type="trend", criterion="MAIC", max.lag.y=8)
adf.DH <-CADFtest(DH, type="drift", criterion="MAIC", max.lag.y=8)
adf.DIR <-CADFtest(DIR, type="drift", criterion="MAIC", max.lag.y=8)
print(adf.H)
print(adf.IR)
print(adf.DH)
print(adf.DIR)
library(CADFtest)
# Unit root test for H, IR, DH and DIR
adf.H <-CADFtest(H, type="drift", criterion="MAIC", max.lag.y=8)
adf.IR <-CADFtest(IR, type="trend", criterion="MAIC", max.lag.y=8)
adf.DH <-CADFtest(DH, type="drift", criterion="MAIC", max.lag.y=8)
adf.DIR <-CADFtest(DIR, type="drift", criterion="MAIC", max.lag.y=8)
print(adf.H)
print(adf.IR)
print(adf.DH)
print(adf.DIR)
b9_h <- c(324, 411)
b9_ir <- c(4.99, 4.97)
b9_date <- c('Feb. 2010', 'Mar. 2010')
b9_headers <- c('Date', 'H', 'IR')
b9 <- data.frame(b9_date, b9_h, b9_ir)
b9_h <- c(324, 411)
b9_ir <- c(4.99, 4.97)
b9_date <- c('Feb. 2010', 'Mar. 2010')
b9_headers <- c('Date', 'H', 'IR')
b9 <- data.frame(b9_date, b9_h, b9_ir)
b9
b9_h <- c(324, 411)
b9_ir <- c(4.99, 4.97)
b9_date <- c('Feb. 2010', 'Mar. 2010')
b9_headers <- c('Date', 'H', 'IR')
b9 <- data.frame(b9_date, b9_h, b9_ir)
colnames(b9) <- b9_headers
b9_h <- c(324, 411)
b9_ir <- c(4.99, 4.97)
b9_date <- c('Feb. 2010', 'Mar. 2010')
b9_headers <- c('Date', 'H', 'IR')
b9 <- data.frame(b9_date, b9_h, b9_ir)
colnames(b9) <- b9_headers
b9
knitr::opts_chunk$set(echo = TRUE)
# Read in the data:
t12 <-read.csv("homes.csv")
# View first few rows of df
head(t12)
# Use the str() function to view the df structure as well as its columns' classes.
str(t12)
# Create the time-series variable:
H <- ts(t12$HOMES, start = c(1992,1), end = c(2010,3), frequency = 12)
IR <- ts(t12$IRATE, start = c(1992,1), end = c(2010,3), frequency = 12)
par(mar = c(4, 4, .1, .1))
# Create first differences of H and IR:
DH <- diff(H)
DIR <- diff(IR)
# Time series plots of H, DH, IR and DIR:
plot(H)
plot(DH)
plot(IR)
plot(DIR)
white_noise<-rnorm(100,0,40)
trend<-3*(1:100)
x_t <- trend+white_noise
plot(x_t,type='l',xlab='Time',ylab=expression('y'[t]),main='Trend Stationary Process')
mu_t<-lm(x_t~c(1:100))
y_t<-x_t-mu_t[['fitted.values']]
plot(y_t,type='l',xlab='Time',ylab=NULL,main='Detrended')
library(CADFtest)
# Unit root test for H, IR, DH and DIR
adf.H <- CADFtest(H, type = "drift", criterion = "MAIC", max.lag.y = 8)
adf.IR <- CADFtest(IR, type = "trend", criterion = "MAIC", max.lag.y = 8)
adf.DH <- CADFtest(DH, type = "drift", criterion = "MAIC", max.lag.y = 8)
adf.DIR <- CADFtest(DIR, type = "drift", criterion = "MAIC", max.lag.y = 8)
print(adf.H)
print(adf.IR)
print(adf.DH)
print(adf.DIR)
acf(DH)
acf(DIR)
setwd("~/Dropbox/2021 Monash Tutoring/ETF5952/Week 1")
# Type your comment
Type your comment
y = c(1,2,3)
y
length(y)
z = c("Tom", "Jack", 100)
z = c("Tom", "Jack", "Mary")
?matrix
v = matrix(data = c(1, 2, 3, 4),
nrow = 2, ncol = 2)
v
v[,1]
v[1,]
v[,2]
v[1,2]
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
v[1,2]
v1 = c("Tom", "Alice", "Alex")
v2 = c(171, 165, 183)
data = data.frame(v1, v2)
View(data)
names(data)
View(data)
names(data)=c("name", "height")
View(data)
data
name
data$name
data$height
data$height[1]
data$height[data$name == "Alex"]
DATA <- read.csv("QuestionnaireData.csv")
names(DATA)
View(DATA)
DATA_males <- DATA[,DATA$Gender == "Male"]
DDATA$Gender == "Male"
DATA$Gender == "Male"
DATA[DATA$Gender == "Male"]
DATA[DATA$Gender == "Male",]
DATA[,DATA$Gender == "Male"]
DATA[;DATA$Gender == "Male"]
View(DATA)
DATA$Gender == "Male"
DATA[,1]
DATA[,]
DATA[DATA$Gender == "Male",]
DATA[DATA$Gender == "Male",]
DATA[DATA$Gender == "Male",]
View(DATA)
DATA_males = DATA[DATA$Gender == "Male",]
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
na <- c(8.5,9.48,8.65, 8.16, 8.83, 7.76 ,8.63)
cau <- c(8.27,8.20, 8.25 , 8.14, 9.00 , 8.10 , 7.20 , 8.32 ,7.70)
dotchart(na)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
qf(0.025, df1=30, df2=22, lower.tail = FALSE)
qf(0.975, df1=30, df2=22, lower.tail = FALSE)
qf(0.025, df1=22, df2=30, lower.tail = FALSE)
qf(0.025, df1=22, df2=30, lower.tail = FALSE)
1/2.16
qf(0.025, df1=22, df2=30, lower.tail = FALSE)
1/2.163
qchisq(0.05, 25, lower.tail = F)
2*pnorm(0.526, lower.tail = F)
setwd("~/Dropbox/2021 Monash Tutoring/ETF5952/Week 8 (tbc)")
setwd("~/Dropbox/2021 Monash Tutoring/ETF5952/Week 8 (tbc)/Rmd")
credit = read.csv("credit.csv")
head(credit)
boxplot(duration ~ Default, data = credit)
boxplot(amount   ~ Default, data = credit)
summary(credit[c(3,6)])
## Convert columns to mean-zero sd-one
x = scale(credit[,c("duration", "amount")])
apply(x, 2, sd)        # 2 indicates columns
apply(x, 2, mean)
library(animation)
## a binary classification problem
ani.options(interval = 1, nmax = 10)
x = matrix(c(rnorm(80, mean = -1), rnorm(80, mean = 1)), ncol = 2,
byrow = TRUE)
y = matrix(rnorm(20, mean = 0, sd = 1.2), ncol = 2)
knn.ani(train = x, test = y, cl = rep(c("A", "B"),
each = 40), k = 30)
set.seed(123) ## seed for random numbers
library(class)
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
## Outcome of this function - factor vector with the predicted classes for
## each row of the TEST data (100 observations)
nn1 = knn(train = x[-test,], test = x[test,], cl = credit$Default[-test], k = 1)
nn1 = knn(train=x[-test,], test=x[test,], cl=credit$Default[-test], k=1)
nn5 = knn(train=x[-test,], test=x[test,], cl=credit$Default[-test], k=5)
nn1 = knn(train=x[-test,], test=x[test,], cl=credit$Default[-test], k=1)
nn5 = knn(train=x[-test,], test=x[test,], cl=credit$Default[-test], k=5)
library(class)
## Step 1: split data into test set. Important to assess model performance
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
## Step 2: estimation which uses the Euclidean distance measure
## Obtain the k nearest neighbours of training set to unknown instance
## Note that k is often an odd number to avoid ties in the 'voting'/majority
## Outcome of this function - factor vector with the predicted classes for
## each row of the TEST data (100 observations)
nn1 = knn(train=x[-test,], test=x[test,], cl=credit$Default[-test], k=1)
nn5 = knn(train=x[-test,], test=x[test,], cl=credit$Default[-test], k=5)
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
## Outcome of this function - factor vector with the predicted classes for
## each row of the TEST data (100 observations)
nn1 = knn(train=x[-test,], test=x[test,], cl=credit$Default[-test], k=1)
credit = read.csv("credit.csv")
set.seed(123) ## seed for random numbers
## borrower and loan characteristics + default outcomes
head(credit)
## boxplots to examine variations in duration and amount predictors
boxplot(duration ~ Default, data = credit)
boxplot(amount   ~ Default, data = credit)
# When calculating distance, units matter! Distance is measured on the raw x
# values and may 'artificially' affect the results.
# Recall the boxplots. What did we observe?
## convert columns to mean-zero sd-one
x = scale(credit[,c("duration", "amount")]) # column 10 is the class label
apply(x,2,sd)        # type "? apply"
apply(x,2,mean)
## k-nn - example of instance-based learning (i.e. new data are classified based
## on stored, labelled instances). Distance between stored data and new instance
## is calculated using a similarity measure expressed by a distance measure
## like the Euclidean distance.
## Similarity to the data is calculated for any new data point that you input
## into the system and use this similarity value to perform predictive modelling
## In k-nn, distance values are sorted and the k-nearest neighbours are determined
## Labels of these neighbours are gathered and majority/weighted vote is used for
## classification or regression purposes
library(class)
## Step 1: split data into test set. Important to assess model performance
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
## Step 2: estimation which uses the Euclidean distance measure
## Obtain the k nearest neighbours of training set to unknown instance
## Note that k is often an odd number to avoid ties in the 'voting'/majority
## Outcome of this function - factor vector with the predicted classes for
## each row of the TEST data (100 observations)
nn1 = knn(train=x[-test,], test=x[test,], cl=credit$Default[-test], k=1)
nn5 = knn(train=x[-test,], test=x[test,], cl=credit$Default[-test], k=5)
credit = read.csv("credit.csv")
head(credit)
boxplot(duration ~ Default, data = credit)
boxplot(amount   ~ Default, data = credit)
summary(credit[c(3,6)])
## Convert columns to mean-zero sd-one
x = scale(credit[,c("duration", "amount")])
apply(x, 2, sd)        # 2 indicates columns
apply(x, 2, mean)
library(animation)
## a binary classification problem
ani.options(interval = 1, nmax = 10)
x1 = matrix(c(rnorm(80, mean = -1), rnorm(80, mean = 1)), ncol = 2,
byrow = TRUE)
y1 = matrix(rnorm(20, mean = 0, sd = 1.2), ncol = 2)
knn.ani(train = x1, test = y1, cl = rep(c("A", "B"),
each = 40), k = 30)
set.seed(123) ## seed for random numbers
library(class)
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
## Outcome of this function - factor vector with the predicted classes for
## each row of the TEST data (100 observations)
nn1 = knn(train = x[-test,], test = x[test,], cl = credit$Default[-test], k = 1)
nn5 = knn(train = x[-test,], test = x[test,], cl = credit$Default[-test], k = 5)
## how many defaults from K-NN prediction
table(nn1)
table(nn5)
## compare model predictions with the actual default values
compare = data.frame(credit$Default[test], nn1, nn5)
credit = read.csv("credit.csv")
head(credit)
boxplot(duration ~ Default, data = credit)
boxplot(amount   ~ Default, data = credit)
summary(credit[c(3,6)])
## Convert columns to mean-zero sd-one
x = scale(credit[,c("duration", "amount")])
apply(x, 2, sd)        # 2 indicates columns
apply(x, 2, mean)
library(animation)
## a binary classification problem
ani.options(interval = 1, nmax = 10)
x1 = matrix(c(rnorm(80, mean = -1), rnorm(80, mean = 1)), ncol = 2,
byrow = TRUE)
y1 = matrix(rnorm(20, mean = 0, sd = 1.2), ncol = 2)
knn.ani(train = x1, test = y1, cl = rep(c("A", "B"),
each = 40), k = 30)
set.seed(123) ## seed for random numbers
library(class)
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
## Outcome of this function - factor vector with the predicted classes for
## each row of the TEST data (100 observations)
nn1 = knn(train = x[-test,], test = x[test,], cl = credit$Default[-test], k = 1)
nn5 = knn(train = x[-test,], test = x[test,], cl = credit$Default[-test], k = 5)
## how many defaults from K-NN prediction
table(nn1)
table(nn5)
## compare model predictions with the actual default values
compare = data.frame(credit$Default[test], nn1, nn5)
## 2 x 2 matrix
mat1 = table(nn1,credit$Default[test])
mat5 = table(nn5,credit$Default[test])
mat1
mat5
paste("For k = 1: True Negative", mat1[1,1], ", False Positive", mat1[2,1],
", False Negative", mat1[1,2], ", True Positive", mat1[2,2])
paste("For k = 5: True Negative", mat5[1,1], ", False Positive", mat5[2,1],
", False Negative", mat5[1,2], ", True Positive", mat5[2,2])
## False positive rate (False Positive / Negative)
FPR1 = mat1[2,1] / (mat1[1,1] + mat1[2,1])
FPR5 = mat5[2,1] / (mat5[1,1] + mat5[2,1])
## False Negative rate (False Negative / Positive)
FNR1 = mat1[1,2] / (mat1[1,2] + mat1[2,2])
FNR5 = mat5[1,2] / (mat5[1,2] + mat5[2,2])
c(FPR1, FPR5)  ## False Positive Rate
c(FNR1, FNR5)  ## False Negative Rate
##------------------------------------------------
## Lasso for spam email
##------------------------------------------------
library(gamlr)
source("roc.R")
credit = read.csv("credit.csv")
head(credit)
boxplot(duration ~ Default, data = credit)
boxplot(amount   ~ Default, data = credit)
summary(credit[c(3,6)])
## Convert columns to mean-zero sd-one
x = scale(credit[,c("duration", "amount")])
apply(x, 2, sd)        # 2 indicates columns
apply(x, 2, mean)
library(animation)
## a binary classification problem
ani.options(interval = 1, nmax = 10)
x1 = matrix(c(rnorm(80, mean = -1), rnorm(80, mean = 1)), ncol = 2,
byrow = TRUE)
y1 = matrix(rnorm(20, mean = 0, sd = 1.2), ncol = 2)
knn.ani(train = x1, test = y1, cl = rep(c("A", "B"),
each = 40), k = 30)
set.seed(123) ## seed for random numbers
library(class)
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
## Outcome of this function - factor vector with the predicted classes for
## each row of the TEST data (100 observations)
nn1 = knn(train = x[-test,], test = x[test,], cl = credit$Default[-test], k = 1)
nn5 = knn(train = x[-test,], test = x[test,], cl = credit$Default[-test], k = 5)
## how many defaults from K-NN prediction
table(nn1)
table(nn5)
## compare model predictions with the actual default values
compare = data.frame(credit$Default[test], nn1, nn5)
## 2 x 2 matrix
mat1 = table(nn1,credit$Default[test])
mat5 = table(nn5,credit$Default[test])
mat1
mat5
paste("For k = 1: True Negative", mat1[1,1], ", False Positive", mat1[2,1],
", False Negative", mat1[1,2], ", True Positive", mat1[2,2])
paste("For k = 5: True Negative", mat5[1,1], ", False Positive", mat5[2,1],
", False Negative", mat5[1,2], ", True Positive", mat5[2,2])
## False positive rate (False Positive / Negative)
FPR1 = mat1[2,1] / (mat1[1,1] + mat1[2,1])
FPR5 = mat5[2,1] / (mat5[1,1] + mat5[2,1])
## False Negative rate (False Negative / Positive)
FNR1 = mat1[1,2] / (mat1[1,2] + mat1[2,2])
FNR5 = mat5[1,2] / (mat5[1,2] + mat5[2,2])
c(FPR1, FPR5)  ## False Positive Rate
c(FNR1, FNR5)  ## False Negative Rate
##------------------------------------------------
## Lasso for spam email
##------------------------------------------------
library(gamlr)
source("roc.R")
set.seed(12345)
## data
email = read.csv("spam.csv")
##------------------------------------------------
## Lasso for spam email
##------------------------------------------------
library(gamlr)
source("roc.R")
set.seed(12345)
## data
email = read.csv("spam.csv")
## summary stat and variables
summary(email)
names(email)
plot(fit)
credit = read.csv("credit.csv")
head(credit)
boxplot(duration ~ Default, data = credit)
boxplot(amount   ~ Default, data = credit)
summary(credit[c(3,6)])
## Convert columns to mean-zero sd-one
x = scale(credit[,c("duration", "amount")])
apply(x, 2, sd)        # 2 indicates columns
apply(x, 2, mean)
library(animation)
## a binary classification problem
ani.options(interval = 1, nmax = 10)
x1 = matrix(c(rnorm(80, mean = -1), rnorm(80, mean = 1)), ncol = 2,
byrow = TRUE)
y1 = matrix(rnorm(20, mean = 0, sd = 1.2), ncol = 2)
knn.ani(train = x1, test = y1, cl = rep(c("A", "B"),
each = 40), k = 30)
set.seed(123) ## seed for random numbers
library(class)
n    = nrow(credit)        ## the full sample size
test = sample(1:n, 0.10*n) ## randomly select 10% observations
table(credit[test,]$Default)
## Outcome of this function - factor vector with the predicted classes for
## each row of the TEST data (100 observations)
nn1 = knn(train = x[-test,], test = x[test,], cl = credit$Default[-test], k = 1)
nn5 = knn(train = x[-test,], test = x[test,], cl = credit$Default[-test], k = 5)
## how many defaults from K-NN prediction
table(nn1)
table(nn5)
## compare model predictions with the actual default values
compare = data.frame(credit$Default[test], nn1, nn5)
## 2 x 2 matrix
mat1 = table(nn1,credit$Default[test])
mat5 = table(nn5,credit$Default[test])
mat1
mat5
paste("For k = 1: True Negative", mat1[1,1], ", False Positive", mat1[2,1],
", False Negative", mat1[1,2], ", True Positive", mat1[2,2])
paste("For k = 5: True Negative", mat5[1,1], ", False Positive", mat5[2,1],
", False Negative", mat5[1,2], ", True Positive", mat5[2,2])
## False positive rate (False Positive / Negative)
FPR1 = mat1[2,1] / (mat1[1,1] + mat1[2,1])
FPR5 = mat5[2,1] / (mat5[1,1] + mat5[2,1])
## False Negative rate (False Negative / Positive)
FNR1 = mat1[1,2] / (mat1[1,2] + mat1[2,2])
FNR5 = mat5[1,2] / (mat5[1,2] + mat5[2,2])
c(FPR1, FPR5)  ## False Positive Rate
c(FNR1, FNR5)  ## False Negative Rate
##------------------------------------------------
## Lasso for spam email
##------------------------------------------------
library(gamlr)
source("roc.R")
set.seed(12345)
## data
email = read.csv("spam.csv")
## summary stat and variables
summary(email)
names(email)
##-----------
## Lasso Regression - recall penalty for complexity
##-----------
## Step 1: define x and y
x = sparse.model.matrix(spam ~ ., data = email)[,-1] # drop intercept term
y = email$spam
n    = nrow(email)             ## sample size
test = sample.int(n, 0.5*n)    ## 50-50 split
## Step 2: estimation
fit    = gamlr(x[-test,], y[-test], family = "binomial") # uses AICc by default
plot(fit)
AICseg = which.min(AICc(fit))
log(fit$lambda[AICseg])
sum(coef(fit)!=0)
fit.cv = cv.gamlr(x[-test,], y[-test], family="binomial",
verb=TRUE)
ggplot(tut2, aes(x=distance)) +
geom_point(aes(y=lnprice, colour="Actual Data")) +
geom_line(aes(y=yhat6, colour="Fitted Values: Log-Linear Model"), size=1) +
labs(x = "Distance from CBD, in kms", y = "(Log) Selling Price, in thousands  dollars") +
scale_colour_manual("",
breaks = c("Actual Data", "Fitted Values: Log-Linear Model"),
values = c("blue", "red")) +
scale_y_continuous(labels = comma) +
theme_classic() +
theme(axis.text=element_text(size=12), axis.title=element_text(size=12))
#----------------------------------------
options(scipen=999)               # Do not use scientific notation
library(ggplot2)                  # Flexible graphic facility for R
\DeclareMathOperator*{\VAR}{VAR} \DeclareMathOperator*{\COV}{COV}
qt(0.05, 17)
# p-value
pt(-1.3658, 17)
